# Deep-Learning-Model-for-Colon-Cancer-Diagnosis
SYSTEM IMPLEMENTATION AND TESTING
INTRODUCTION
The aim of this study is to develop a deep learning model for colon cancer disease diagnosis using tissue images. This model aims to accurately differentiate between malignant (Colon Adenocarcinoma) and benign colon tissue. This chapter delves deep into the implementation and testing of the model. The implementation focuses on data preparation (pre-processing), the various sections and systems of the model, their implementation and functionality, and the tools used in the implementation. Testing focuses on the verification of the model’s ability to satisfy the functional and usability requirements.
SYSTEM IMPLEMENTATION
Development Environment
The model development was carried out on Google Colab, which serves as the integrated development environment for the model's codebase. This platform was preferred because it provides access to essential Python frameworks required for the model. Google Colab, integrated within a web browser, allows for quick implementation and development without the need for a full-fledged code editor. Additionally, the platform was chosen for its capability to utilize hardware acceleration of the GPU (Graphical Processing Unit), essential for efficiently training the model with TensorFlow. Google Colab also offers access to powerful libraries such as TensorFlow, OpenCV, and various Python tools. Furthermore, it provides automated backup of notebooks containing the code and image datasets, ensuring data security and easy access.
The software resources included the programming language and libraries used for the model development. Python was the primary language utilized. The libraries used were common in machine learning and computer vision, providing various functionalities necessary for the model development. The environment for code writing and editing was Google Colab, which offers a Jupyter notebook interface. This platform facilitated the integration and use of powerful libraries such as TensorFlow and OpenCV, ensuring an efficient and streamlined development process.

Software	Version
Google Colab Platform	
Python	3.9.13
Jupyter Notebook (via Google Colab)	
OpenCV	3.4.3
TensorFlow	2.15.0

Collecting the image dataset
The source of our image dataset is Kaggle. In a web-based data science environment, Kaggle enables users to explore, construct models, and find and publish datasets. It is a repository of published datasets that can be used by any user for studies and research. An account on Kaggle was created with an email address and a password. These credentials were used to log in and download the colon cancer image dataset. The dataset comprises 10,000 images of colon tissue, with 5,000 images of benign colon tissue and 5,000 images of colon adenocarcinomas. This dataset, derived from HIPAA-compliant and validated sources (Borkowski, Bui, Thomas, Wilson, DeLand & Mastorides, 2019), was downloaded in preparation for the preprocessing steps ahead. The URL for Kaggle is https://www.kaggle.com.
Visualize the image using matplot
To better understand the dataset and ensure that the images are correctly labeled and preprocessed, we visualized a subset of the images using Matplotlib. A grid of randomly selected images from each class was displayed, allowing us to inspect the images and their corresponding labels. This visualization step helps in verifying that the images are correctly labeled and provides an overview of the dataset's content, which is crucial for effective model training and evaluation.
 
Segmenting the images into training set, validation set and testing sets
Before augmentation, the dataset was split into three sets with a ratio of 0.80 for training, 0.10 for testing, and 0.10 for validation. This split ensured that the model had a substantial amount of data for training while reserving adequate samples for testing and validation. Images were then visualized from each set to verify their distribution. Figure 7 illustrates the visualized samples from the dataset. Additionally, we created a list of indexes for each data set, producing an output summary graph that displayed the total number of images in each set, providing a clear overview of the dataset distribution.
 
Data preprocessing
Data preprocessing is crucial for developing an accurate deep-learning model. In this study, all images were resized to 300x300 pixels to ensure consistency and compatibility with the EfficientNetB3 model. The resizing and various image augmentation techniques were performed using TensorFlow's Keras ‘ImageDataGenerator’. Data augmentation is a set of techniques employed to increase the number of datasets by generating additional samples through various transformations. This process helps enhance the accuracy and generalization of deep learning models by expanding the quantity and diversity of the datasets. Additionally, data augmentation aids in preventing overfitting during the training process, thereby enhancing the robustness and stability of the models. In this study, augmentation techniques included rotation (up to 20 degrees), width and height shifts (up to 20% of the total width/height), shearing, zooming (up to 20%), and horizontal flipping. These transformations generated new, varied versions of the original images, which improved the model's ability to generalize. By enhancing the dataset with these augmented images, the model's robustness and accuracy in diagnosing colon cancer from tissue images were significantly improved. 
Feature extraction
Feature extraction with deep learning involves the automated process of learning and extracting significant features from raw data using deep neural networks. This process is crucial for enabling deep learning models to perform various tasks, such as object recognition, image classification, and natural language processing. Deep learning models, including Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Restricted Boltzmann Machines (RBMs), and Recurrent Neural Networks (RNNs), are designed to automatically learn and extract features from input data. These models utilize multiple layers to identify and capture relevant features at different levels of abstraction. In this study, a pre-trained CNN model was employed for feature extraction. These model leverages its deep architecture to efficiently and effectively extract features from the dataset, enhancing the performance of the classification task. An overview of the chosen model and its respective contributions to feature extraction will be presented in the subsequent sections.
MODEL DEVELOPMENT AND TRAINING
Model Architecture and Training Procedure
Model Architecture (EfficientNetB3 with Custom Layers): The deep learning model integrates the EfficientNetB3 architecture with custom layers to enhance binary classification tasks. EfficientNetB3, a state-of-the-art convolutional neural network known for its parameter efficiency and high accuracy, serves as the base model. It employs a compound scaling method to balance depth, width, and resolution, optimizing both accuracy and efficiency (Tan et al., 2019). The EfficientNetB3 model processes input images of 300x300 pixels and includes several convolutional layers and depthwise separable convolutions, which are crucial for high-level feature extraction.
In this custom hybrid model, the top classification layers of EfficientNetB3 are removed (include_top=False), and it is initialized with pre-trained weights from ImageNet. This setup leverages learned features from a diverse dataset. The output from EfficientNetB3 is passed through a Global Average Pooling layer, which aggregates spatial dimensions of the feature maps and reduces them to a single vector per image. This step preserves critical features while managing dimensionality. A Dense layer with 1024 units and ReLU activation follows, enabling the model to learn complex patterns by mapping pooled features to a higher-dimensional space. The final Dense layer, with a single unit and sigmoid activation function, is used for binary classification, producing a probability score between 0 and 1 to indicate the presence or absence of the condition.
Binary Cross-Entropy Loss: The model employs binary cross-entropy as the loss function to evaluate performance in binary classification tasks. This loss function measures the discrepancy between the predicted probability and the actual class label. 

Training Procedure: The model is compiled using the Adamax optimizer for its stability and efficiency in handling sparse gradients (Kingma & Ba, 2014). An adaptive learning rate was utilized to minimize the objective function of the network while avoiding overfitting. This approach can be considered an optimization of gradient descent methods. The objective function of the network was lowered by leveraging the network’s parameters and the gradient of the function. The loss function utilized was binary cross-entropy, suitable for binary classification problems. Performance metrics including accuracy, precision, recall, AUC, Mean Absolute Error, Mean Squared Error, and Root Mean Squared Error were employed to comprehensively evaluate the model’s effectiveness. 
To prevent overfitting and ensure optimal performance, early stopping and model checkpointing are employed. Early stopping monitors validation loss and halts training if no improvement is observed for 10 epochs, while model checkpointing saves the best weights based on validation loss. The model is trained with 8000 images for 50 epochs with a batch size of 32, incorporating image data augmentation to enhance robustness and prevent overfitting. The combination of EfficientNetB3 with custom dense layers and the use of binary cross-entropy loss results in a robust model architecture capable of effective binary classification, demonstrating the advantages of transfer learning and advanced neural network design.
MODEL TESTING
Model Deployment and Validation
The model deployment was done on a web application, using a front-end React framework and a Flask backend. This setup allows the model to be accessible by the public and not just the developer. The essence of having a web-based user interface is to package the application in a manner that is usable by the users; in this case, medical practitioners or researchers who want to diagnose colon cancer from tissue images. The backend handles the user login logic and the engine for the model logic, while the front-end handles the uploading of images and displaying the prediction results.
 
Login Interface

To achieve this, the model was deployed using TensorFlow and Flask on the backend, which allows the model to be interfaced with the web application. The Flask backend loads the TensorFlow model and processes the uploaded images to make predictions. The web browser interface enables users to upload colon tissue images, which are then processed by the model to predict whether the tissue is benign or malignant. The predictions are displayed in real-time, providing immediate feedback to the users.


The user can choose a file and click the predict button, which prompts the model to analyze the uploaded colon tissue image. The model predicts whether the tissue is benign or malignant based on the training dataset. The prediction is displayed as a textual diagnosis below the image. This allows users to understand the diagnosis by reading the text on the screen.
Hyperparameter Fine Tuning
An experimental study approach was foundational for constructing the deep learning model. To optimize the model's performance, several parameters were adjusted, notably the learning rate and the number of epochs. The initial model development focused on using EfficientNetB3 with a fixed learning rate and a standard set of metrics. Subsequently, the model underwent multiple levels of fine-tuning, including adjusting the model to be trainable and lowering the learning rate to improve detection accuracy for diagnosing colon tissue images. This process was facilitated by ample time for testing and varying the parameters, ensuring the model achieved the desired levels of accuracy and robustness.
